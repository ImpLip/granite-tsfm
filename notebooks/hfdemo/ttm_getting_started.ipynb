{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7478e0e2-b7af-4fd4-b44e-ca58e0c31b71",
   "metadata": {},
   "source": [
    "# Getting started with TinyTimeMixer (TTM)\n",
    "\n",
    "This notebooke demonstrates the usage of a pre-trained `TinyTimeMixer` model for several multivariate time series forecasting tasks. For details related to model architecture, refer to the [TTM paper](https://arxiv.org/pdf/2401.03955.pdf).\n",
    "\n",
    "In this example, we will use a pre-trained TTM-512-96 model. That means the TTM model can take an input of 512 time points (`context_length`), and can forecast upto 96 time points (`forecast_length`) in the future. We will use the pre-trained TTM in two settings:\n",
    "1. **Zero-shot**: The pre-trained TTM will be directly used to evaluate on the `test` split of the target data. Note that the TTM was NOT pre-trained on the target data.\n",
    "2. **Few-shot**: The pre-trained TTM will be quickly fine-tuned on only 5% of the `train` split of the target data, and subsequently, evaluated on the `test` part of the target data.\n",
    "\n",
    "Note: Alternatively, this notebook can be modified to try the TTM-1024-96 model.\n",
    "\n",
    "Pre-trained TTM models will be fetched from the [Hugging Face TTM Model Repository](https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63ae353-96df-4380-89f6-1e6cebf684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "# Add the parent directory of tsfm_public to the Python path\n",
    "sys.path.append('/Users/chouchou/Code/MachineLearning/granite-tsfm')\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import TinyTimeMixerForPrediction, TrackingCallback, count_parameters, load_dataset\n",
    "from tsfm_public.toolkit.visualization import plot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894ac389-94e4-4956-8d09-6509d9d452e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "092f5fa8-7f21-46d5-8356-2f313276d345",
   "metadata": {},
   "source": [
    "### Important arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a826c4f3-1c6c-4088-b6af-f430f45fd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# DATA ROOT PATH\n",
    "# Make sure to download the target data (here ettm2) on the `DATA_ROOT_PATH` folder.\n",
    "# ETT is available at: https://github.com/zhouhaoyi/ETDataset/tree/main\n",
    "target_dataset = \"ettm2\"\n",
    "DATA_ROOT_PATH = \"/Users/chouchou/Code/MachineLearning/granite-tsfm/notebooks/data\"\n",
    "\n",
    "# Results dir\n",
    "OUT_DIR = \"ttm_finetuned_models/\"\n",
    "\n",
    "# TTM model branch\n",
    "# Use main for 512-96 model\n",
    "# Use \"1024_96_v1\" for 1024-96 model\n",
    "TTM_MODEL_REVISION = \"main\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9498749",
   "metadata": {},
   "source": [
    "## Zero-shot evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7935d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_eval(dataset_name, batch_size, context_length=512, forecast_length=96, prediction_filter_length=None):\n",
    "    # Get data\n",
    "    _, _, dset_test = load_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        context_length=context_length,\n",
    "        forecast_length=forecast_length,\n",
    "        fewshot_fraction=1.0,\n",
    "        dataset_root_path=DATA_ROOT_PATH,\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    if prediction_filter_length is None:\n",
    "        zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "            \"ibm-granite/granite-timeseries-ttm-v1\", revision=TTM_MODEL_REVISION\n",
    "        )\n",
    "    else:\n",
    "        if prediction_filter_length <= forecast_length:\n",
    "            zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                \"ibm-granite/granite-timeseries-ttm-v1\",\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # zeroshot_trainer\n",
    "    zeroshot_trainer = Trainer(\n",
    "        model=zeroshot_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=temp_dir,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            seed=SEED,\n",
    "        ),\n",
    "    )\n",
    "    # evaluate = zero-shot performance\n",
    "    print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "    zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "    print(zeroshot_output)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=zeroshot_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_zeroshot\",\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b79e3b8-b80a-48c4-9ee7-810e9ebdfcd2",
   "metadata": {},
   "source": [
    " ## Few-shot finetune and evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078c945a-9da7-4729-a95d-43cd615d0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_finetune_eval(\n",
    "    dataset_name,\n",
    "    batch_size,\n",
    "    learning_rate=0.001,\n",
    "    context_length=512,\n",
    "    forecast_length=96,\n",
    "    fewshot_percent=5,\n",
    "    freeze_backbone=True,\n",
    "    num_epochs=50,\n",
    "    save_dir=OUT_DIR,\n",
    "    prediction_filter_length=None,\n",
    "):\n",
    "    out_dir = os.path.join(save_dir, dataset_name)\n",
    "\n",
    "    print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "    dset_train, dset_val, dset_test = load_dataset(\n",
    "        dataset_name,\n",
    "        context_length,\n",
    "        forecast_length,\n",
    "        fewshot_fraction=fewshot_percent / 100,\n",
    "        dataset_root_path=DATA_ROOT_PATH,\n",
    "    )\n",
    "\n",
    "    # change head dropout to 0.7 for ett datasets\n",
    "    if \"ett\" in dataset_name:\n",
    "        if prediction_filter_length is None:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                \"ibm-granite/granite-timeseries-ttm-v1\", revision=TTM_MODEL_REVISION, head_dropout=0.7\n",
    "            )\n",
    "        elif prediction_filter_length <= forecast_length:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                \"ibm-granite/granite-timeseries-ttm-v1\",\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                head_dropout=0.7,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    else:\n",
    "        if prediction_filter_length is None:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                \"ibm-granite/granite-timeseries-ttm-v1\",\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "            )\n",
    "        elif prediction_filter_length <= forecast_length:\n",
    "            finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "                \"ibm-granite/granite-timeseries-ttm-v1\",\n",
    "                revision=TTM_MODEL_REVISION,\n",
    "                prediction_filter_length=prediction_filter_length,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"`prediction_filter_length` should be <= `forecast_length\")\n",
    "    if freeze_backbone:\n",
    "        print(\n",
    "            \"Number of params before freezing backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "        # Freeze the backbone of the model\n",
    "        for param in finetune_forecast_model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Count params\n",
    "        print(\n",
    "            \"Number of params after freezing the backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "    print(f\"Using learning rate = {learning_rate}\")\n",
    "    finetune_forecast_args = TrainingArguments(\n",
    "        output_dir=os.path.join(out_dir, \"output\"),\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        dataloader_num_workers=8,\n",
    "        report_to=None,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Create the early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "        early_stopping_threshold=0.0,  # Minimum improvement required to consider as improvement\n",
    "    )\n",
    "    tracking_callback = TrackingCallback()\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
    "    )\n",
    "\n",
    "    finetune_forecast_trainer = Trainer(\n",
    "        model=finetune_forecast_model,\n",
    "        args=finetune_forecast_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_val,\n",
    "        callbacks=[early_stopping_callback, tracking_callback],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "\n",
    "    # Fine tune\n",
    "    finetune_forecast_trainer.train()\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
    "    fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "    print(fewshot_output)\n",
    "    print(\"+\" * 60)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=finetune_forecast_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_fewshot\",\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b474d86",
   "metadata": {},
   "source": [
    "## Example: downstream target dataset - ettm2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0d40b66",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a84d458-76ca-4e2a-a756-59981e9847f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-34564:t-8593203712:data_handling.py:load_dataset:Dataset name: ettm2, context length: 512, prediction length 96\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'notebooks/data/ETT-small/ETTm2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mzeroshot_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mzeroshot_eval\u001b[0;34m(dataset_name, batch_size, context_length, forecast_length, prediction_filter_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeroshot_eval\u001b[39m(dataset_name, batch_size, context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, forecast_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, prediction_filter_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Get data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     _, _, dset_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforecast_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforecast_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfewshot_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_root_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_ROOT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_filter_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/MachineLearning/granite-tsfm/venv/lib/python3.12/site-packages/tsfm_public/toolkit/data_handling.py:66\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(dataset_name, context_length, forecast_length, fewshot_fraction, fewshot_location, dataset_root_path, dataset_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m Path(dataset_root_path) \u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp_column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m train_dataset, valid_dataset, test_dataset \u001b[38;5;241m=\u001b[39m get_datasets(\n\u001b[1;32m     72\u001b[0m     tsp,\n\u001b[1;32m     73\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     fewshot_location\u001b[38;5;241m=\u001b[39mfewshot_location,\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData lengths: train = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/MachineLearning/granite-tsfm/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/MachineLearning/granite-tsfm/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Code/MachineLearning/granite-tsfm/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/MachineLearning/granite-tsfm/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Code/MachineLearning/granite-tsfm/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'notebooks/data/ETT-small/ETTm2.csv'"
     ]
    }
   ],
   "source": [
    "zeroshot_eval(dataset_name=target_dataset, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e825cf28-f034-4a32-a729-0fe846ff2a26",
   "metadata": {},
   "source": [
    "### Few-shot 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145fead-50fb-4e3e-89fc-a0c238755e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(dataset_name=target_dataset, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cee2dcc1-bcb8-47ee-8ba7-ff3104159ed6",
   "metadata": {},
   "source": [
    "## Example: Automatically truncating the forecast horizon\n",
    "\n",
    "Here, we demonstrate that a pre-trained 512-96 TTM model (i.e., context length = 512, forecast horizon = 96) \n",
    "can be used for a task having forecast horizon less than 96 time points.\n",
    "We need to specify the argument `prediction_filter_length` while loading the model. That's it!\n",
    "\n",
    "Note that the model performance might be sacrificed by some margin while truncating the model forecast. It is recommended to try \n",
    "this feature in your validation data for your experiment, to verify if the model performance is in the acceptable threshold. \n",
    "Otherwise, a new TTM model can be pre-trained with the required forecast horizon.\n",
    "\n",
    "In this example, we will use a 512-96 TTM and use it on ettm2 data for forecasting 48 points in both zero-shot and 5% few-shot settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31b58250-91a2-4db0-ab45-29b89f5afd0e",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eff2e1-acfd-4c5b-8463-e084ba831cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_eval(dataset_name=target_dataset, batch_size=64, prediction_filter_length=48)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00e56d63-5e87-41ae-8fe9-3b3cfd9d19f6",
   "metadata": {},
   "source": [
    "### Few-shot 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56cd24-bae6-4cc6-9a3c-52f965014eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(dataset_name=target_dataset, batch_size=64, prediction_filter_length=48)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
